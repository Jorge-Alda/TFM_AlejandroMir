{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Test de hipótesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una tarea de la lección 1 intentábamos buscar los valores de ciertos coeficientes de Wilson que reprodujeran los valores experimentales de algunos observables. En aquella tarea nos limitamos a buscar una aproximación, pero ahora vamos a identificar la respuesta exacta. Para ello, deberemos:\n",
    "\n",
    "* Cuantificar cómo de bueno es un conjunto de parámetros (en nuestro caso, coeficientes de Wilson) a la hora de describir los resultados experimentales.\n",
    "* Obtener los parámetros que mejor describen los datos experimentales.\n",
    "* Decidir si el resultado obtenido es significativamente mejor que el SM (hipótesis nula)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de pérdida\n",
    "\n",
    "\n",
    "Nuestro objetivo es determinar $n$ parámetros $\\vec{\\theta}$ definidos en una región $R \\subseteq \\mathbb{R}^n$. Estos parámetros los emplearemos en un modelo $f_i(\\vec{\\theta})$ para hacer predicciones $x_i = f_i(\\vec{\\theta})$ que contrastamos con las observaciones $y_i$.\n",
    "\n",
    "En nuestro caso, los parámetros $\\vec{\\theta}$ serán los coeficientes de Wilson, el modelo será el SMEFT, los valores de $x_i$ serán ciertos observables ($R_{K^{(*)}}$, $R_{D^{(*)}}$, $R_{J/\\psi}$, ...) que calcularemos mediante la función `np_prediction` de `flavio`, y las observaciones $y_i$ serán las mediciones experimentales para esos mismos observables.\n",
    "\n",
    "Una función de pérdida o de coste sirve para cuantificar cómo de bien los parámetros $\\vec{\\theta}$ describen las observaciones $y_i$. Se trata de una función $L: R \\to \\mathbb{R}$ que tenga un mínimo cuando los valores predichos por el modelo sean iguales a las observaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las funciones de pérdida más comunes es la función cuadrática: $$L(\\vec{\\theta}) = \\sum_i (x_i - y_i)^2 = \\sum_i (f_i(\\vec{\\theta}) - y_i)^2.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la función cuadrática en acción. En este caso, incluiremos dos observables relacionados con los procesos $b\\to s\\ell\\ell$, $R_K$ y $\\frac{d\\mathrm{BR}}{dq^2}(B\\to K\\mu^+\\mu^-)$, ambos en el bin $q^2 \\in [1.1,\\ 6.0]\\ \\mathrm{GeV}^2$. El modelo tendrá dos parámetros $K_1$ y $K_2$, y en vez de usar el SMEFT, simplemente sumaremos $K_i$ a las predicciones del SM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flavio\n",
    "\n",
    "RK_SM = flavio.sm_prediction(\"<Rmue>(B+->Kll)\", 1.1, 6.0)\n",
    "dBR_SM = flavio.sm_prediction(\"<dBR/dq2>(B+->Kmumu)\", 1.1, 6.0)\n",
    "\n",
    "RK_exp = flavio.combine_measurements((\"<Rmue>(B+->Kll)\", 1.1, 6.0)).central_value\n",
    "dBR_exp = flavio.combine_measurements((\"<dBR/dq2>(B+->Kmumu)\", 1.1, 6.0)).central_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo(K1, K2):\n",
    "    return RK_SM + K1, dBR_SM + K2\n",
    "\n",
    "\n",
    "def loss(modelo, pars):\n",
    "    y = (RK_exp, dBR_SM)\n",
    "\n",
    "    return sum((xi-yi)**2 for xi, yi in zip(modelo(*pars), y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el valor de la función de pérdida para $\\vec{K} = (0.1, 0)$ y para $\\vec{K} = (0, 0.1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06303582803555643"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(modelo, (0.1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0328219542413324"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(modelo, (0, 0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según la función de pérdida que estamos usando, el segundo conjunto de parámetros es mejor que el primero, ya que tiene menor pérdida. Sin embargo, si examinamos las predicciones del modelo en ambos puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.1007790786808298, 3.491200391085207e-08)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo(0.1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0007790786808297, 0.10000003491200392)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo(0, 0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el primer caso, estamos modificando el valor de $R_K$ en un 10%, mientras que en el segundo, hemos aumentado el *branching ratio* diferencial en 7 órdenes de magnitud!\n",
    "\n",
    "La función de pérdida que hemos empleado no funciona bien cuando tenemos observables que abarcan varios órdenes de magnitud. Podemos corregirla si normalizamos los observables. La elección natural para la normalización es la incertidumbre de cada observable.$$L(\\vec{\\theta}) = \\sum_i \\frac{(x_i - y_i)^2}{\\sigma_i^2} = \\sum_i \\frac{(f_i(\\vec{\\theta}) - y_i)^2}{\\sigma_i^2}.$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la práctica, asumiremos que la incertidumbre tiene dos componentes independientes, experimental y teórica, y que esta última se puede aproximar por la incertidumbre teórica en el SM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RK_unc_SM = flavio.sm_uncertainty(\"<Rmue>(B+->Kll)\", 1.1, 6.0)\n",
    "dBR_unc_SM = flavio.sm_uncertainty(\"<dBR/dq2>(B+->Kmumu)\", 1.1, 6.0)\n",
    "\n",
    "RK_unc_exp = flavio.combine_measurements((\"<Rmue>(B+->Kll)\", 1.1, 6.0)).error_left\n",
    "dBR_unc_exp = flavio.combine_measurements((\"<dBR/dq2>(B+->Kmumu)\", 1.1, 6.0)).error_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(modelo, pars):\n",
    "    y = (RK_exp, dBR_SM)\n",
    "    sigma2 = (RK_unc_SM**2+RK_unc_exp**2, dBR_unc_SM**2+dBR_unc_exp**2)\n",
    "\n",
    "    return sum((xi-yi)**2/s2i for xi, yi, s2i in zip(modelo(*pars), y, sigma2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta nueva función de pérdida, obtenemos resultados más coherentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.519297254552264"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(modelo, (0.1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228238794229687.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(modelo, (0, 0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una última complicación ocurre cuando varios observables no son estadísticamente independientes, ya sea porque su determinación teórica está relacionada (como por ejemplo $R_K$ y $\\frac{d\\mathrm{BR}}{dq^2}$) o porque las mediciones experimentales emplean el mismo análisis ($R_D$ y $R_{D^*}$). En estos casos, utilizar la desviación estadística de cada observable individualmente hará que se sobreestime la pérdida. En su lugar hay que usar la matriz de covarianza $C$,\n",
    "$$L(\\vec{\\theta}) = \\sum_{i,j} (x_i - y_i) C^{-1}_{ij} (x_j - y_j) = \\sum_{i,j} (f_i(\\vec{\\theta}) - y_i)C^{-1}_{ij}(f_j(\\vec{\\theta}) - y_j).$$\n",
    "\n",
    "La covariancia de dos variaables estadísticas se calcula como $C_{ij} = E[(x_i-E[x_i])(x_j-E[x_j])]$, donde $E[]$ denota el valor esperado. Por lo tanto, los elementos de la diagonal se corresponden con la varianza $C_{ii} = \\sigma_i^2$, y en los casos en los que las variables $x_i$ y $x_j$ sean estadísticamente independientes, los términos fuera de la diagonal $C_{ij} = 0$ y la función de pérdida se reduce al caso anterior.\n",
    "\n",
    "Relacionada con la covarianza se puede definir la correlación, $\\mathrm{Corr}_{ij} = \\frac{C_{ij}}{\\sigma_i\\sigma_j}$. Los elementos de la diagonal de la matriz de correlación siempre son iguales a 1, y el resto están comprendidos entre -1 y 1, siendo una correlación nula si las variables son estadísticamente independientes, y $\\pm1$ si tienen una relación lineal $x_1 = m x_2$ ($+1$ si $m> 0$ y $-1$ si $m < 0$).\n",
    "\n",
    "En `flavio` la covarianza en el SM de varios observables se calcula con la función `sm_covariance`, a la que hay que pasarle una lista con los observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_SM = flavio.sm_covariance([(\"<Rmue>(B+->Kll)\", 1.1, 6.0), (\"<dBR/dq2>(B+->Kmumu)\", 1.1, 6.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3844541118682048"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_SM[0,1]/(RK_unc_SM*dBR_unc_SM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a calcular la nueva función de pérdida. Para hacer las operaciones con vectores y matrices usaremos `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(modelo, pars):\n",
    "    inv_cov = np.matrix(cov_SM).I\n",
    "    y = (RK_exp, dBR_SM)\n",
    "    residuals = np.array([xi-yi for xi, yi in zip(modelo(*pars), y)])\n",
    "    return float(residuals @ inv_cov @ residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869484.9800030973"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(modelo, (0.1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276193237137052.28"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(modelo, (0, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm-alejandromir-pisT7Re7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f3f3a4d53262e9b64aadf354276afdb3b72bef7e271f3b7e5577e448e334dcb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
